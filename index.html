<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Contrôle LED - MediaPipe</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>
  <style>
    body { margin: 0; display: flex; justify-content: center; align-items: center; height: 100vh; background: #000; }
    video, canvas { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%) scaleX(-1); }
    #status { position: absolute; top: 10px; left: 10px; background: rgba(0,0,0,0.6); color: white; padding: 8px; font-size: 20px; z-index: 10; }
  </style>
</head>
<body>
  <video id="video" width="640" height="480" autoplay muted playsinline></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <div id="status">Chargement...</div>

  <script>
    const LED_URL = "http://172.20.10.12/led?cmd=";  
    let gestureBuffer = [];
    const gestureThreshold = 3;
    let lastCmd = null;
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const statusBox = document.getElementById('status');

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 }, audio: false });
      video.srcObject = stream;
      return new Promise(resolve => video.onloadedmetadata = () => resolve(video));
    }

    async function main() {
      await tf.setBackend('webgl');
      await setupCamera();
      video.play();

      const detector = await handPoseDetection.createDetector(handPoseDetection.SupportedModels.MediaPipeHands, {
        runtime: 'tfjs', modelType: 'lite', maxHands: 1
      });

      statusBox.innerText = "Modèle prêt - montre ta main";

      setInterval(async () => {
        const hands = await detector.estimateHands(video, { flipHorizontal: true });
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (hands.length > 0) {
          const keypoints = hands[0].keypoints;
          keypoints.forEach(pt => {
            ctx.beginPath();
            ctx.arc(pt.x, pt.y, 5, 0, 2 * Math.PI);
            ctx.fillStyle = "lime";
            ctx.fill();
          });

          const dx = keypoints[4].x - keypoints[8].x;
          const dy = keypoints[4].y - keypoints[8].y;
          const distance = Math.sqrt(dx * dx + dy * dy);

          let cmd = null;
          if (distance < 40) { cmd = "off"; statusBox.innerText = "Geste : OFF"; }
          else if (distance < 80) { cmd = "blink"; statusBox.innerText = "Geste : BLINK"; }
          else { cmd = "on"; statusBox.innerText = "Geste : ON"; }

          if (cmd) {
            gestureBuffer.push(cmd);
            if (gestureBuffer.length > gestureThreshold) gestureBuffer.shift();
            if (gestureBuffer.length === gestureThreshold && gestureBuffer.every(c => c === cmd) && cmd !== lastCmd) {
              lastCmd = cmd;
              fetch(LED_URL + cmd).then(() => console.log("Commande envoyée :", cmd));
            }
          }
        } else {
          statusBox.innerText = "Aucune main détectée";
          gestureBuffer = [];
        }
      }, 300);
    }
    main();
  </script>
</body>
</html>
